import axios from 'axios';
import chalk from 'chalk';
import ora from 'ora';
import { CommandOptions } from '../types';

const AI_SERVICE_URL = process.env['AI_SERVICE_URL'] || 'http://localhost:8000';

export async function setupCommand(options: CommandOptions) {
  const spinner = ora('Setting up AI service...').start();
  
  try {
    // Call the AI service setup endpoint
    const response = await axios.post(`${AI_SERVICE_URL}/setup`, {
      codebase_path: options.path || '.'
    });

    const result = response.data;
    
    spinner.succeed('AI service setup completed!');
    
    console.log('\n' + chalk.blue.bold('Setup Results:'));
    console.log(chalk.gray('‚îÄ'.repeat(50)));
    
    // Display LLM status
    if (result.llm_status) {
      const llmStatus = result.llm_status.status === 'healthy' 
        ? chalk.green('‚úì Healthy') 
        : chalk.red('‚úó Unavailable');
      console.log(`LLM Service: ${llmStatus}`);
      
      if (result.llm_status.status !== 'healthy') {
        console.log(chalk.yellow('  ‚Üí Make sure Ollama is running with the specified model'));
      }
    }
    
    // Display indexing status
    if (result.indexing_status) {
      if (result.indexing_status.error) {
        console.log(`Codebase Indexing: ${chalk.red('‚úó Failed')}`);
        console.log(chalk.red(`  ‚Üí ${result.indexing_status.error}`));
      } else {
        console.log(`Codebase Indexing: ${chalk.green('‚úì Completed')}`);
        if (result.indexing_status.files_processed) {
          console.log(`  ‚Üí Processed ${result.indexing_status.files_processed} files`);
        }
      }
    }
    
    // Display overall status
    console.log('\n' + chalk.bold('Overall Status:'));
    const overallStatus = result.overall_status === 'ready' 
      ? chalk.green('‚úì Ready') 
      : chalk.yellow('‚ö† Limited');
    console.log(`Service: ${overallStatus}`);
    
    if (result.overall_status === 'ready') {
      console.log(chalk.green('\nüéâ AI service is ready to generate and analyze tests!'));
    } else if (result.overall_status === 'llm_unavailable') {
      console.log(chalk.yellow('\n‚ö†Ô∏è  LLM is unavailable. Some features will be limited.'));
      console.log(chalk.gray('  ‚Üí Start Ollama: ollama run mistral'));
    } else {
      console.log(chalk.yellow('\n‚ö†Ô∏è  Setup completed with some issues.'));
    }
    
  } catch (error) {
    spinner.fail('Setup failed');
    
    if (axios.isAxiosError(error)) {
      if (error.code === 'ECONNREFUSED') {
        console.log(chalk.red('\n‚ùå Cannot connect to AI service.'));
        console.log(chalk.yellow('  ‚Üí Make sure the AI service is running:'));
        console.log(chalk.gray('    cd ai-automation/ai-service/src'));
        console.log(chalk.gray('    ../venv/bin/uvicorn api_server:app --reload --host 0.0.0.0 --port 8000'));
      } else {
        console.log(chalk.red(`\n‚ùå API Error: ${error.response?.data?.detail || error.message}`));
      }
    } else {
      console.log(chalk.red(`\n‚ùå Unexpected error: ${error}`));
    }
    
    process.exit(1);
  }
} 